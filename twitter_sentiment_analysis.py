# -*- coding: utf-8 -*-
"""TWITTER SENTIMENT ANALYSIS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sG2KJMxREweBAb-lzpIgBvst2t9xghuZ
"""

import pandas as pd
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob
from collections import Counter
import nltk

class TwitterSentimentAnalyzer:
    def __init__(self):
        """Initialize the sentiment analyzer with required NLTK downloads"""
        # Download required NLTK data
        try:
            nltk.download('punkt')
            nltk.download('stopwords')
            nltk.download('wordnet')
            nltk.download('averaged_perceptron_tagger')
        except:
            print("NLTK data already downloaded")

        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))

    def preprocess_tweet(self, tweet):
        """Clean and preprocess the tweet text"""
        # Convert to lowercase
        tweet = tweet.lower()

        # Remove URLs
        tweet = re.sub(r'http\S+|www\S+|https\S+', '', tweet, flags=re.MULTILINE)

        # Remove user mentions
        tweet = re.sub(r'@\w+', '', tweet)

        # Remove hashtags
        tweet = re.sub(r'#\w+', '', tweet)

        # Remove numbers and special characters
        tweet = re.sub(r'[^\w\s]', '', tweet)
        tweet = re.sub(r'\d+', '', tweet)

        # Tokenization
        tokens = word_tokenize(tweet)

        # Remove stopwords and lemmatize
        tokens = [self.lemmatizer.lemmatize(token) for token in tokens
                 if token not in self.stop_words and len(token) > 2]

        return ' '.join(tokens)

    def analyze_sentiment(self, tweet):
        """Analyze sentiment of a single tweet"""
        # Preprocess the tweet
        processed_tweet = self.preprocess_tweet(tweet)

        # Get sentiment using TextBlob
        analysis = TextBlob(processed_tweet)

        # Get polarity score (-1 to 1)
        polarity = analysis.sentiment.polarity

        # Determine sentiment category
        if polarity > 0:
            sentiment = 'positive'
        elif polarity < 0:
            sentiment = 'negative'
        else:
            sentiment = 'neutral'

        return {
            'original_tweet': tweet,
            'processed_tweet': processed_tweet,
            'polarity': polarity,
            'sentiment': sentiment
        }

    def analyze_topic_sentiment(self, tweets, topic=None):
        """Analyze sentiment for a collection of tweets, optionally filtered by topic"""
        results = []

        # Filter by topic if specified
        if topic:
            topic = topic.lower()
            tweets = [tweet for tweet in tweets if topic in tweet.lower()]

        # Analyze each tweet
        for tweet in tweets:
            results.append(self.analyze_sentiment(tweet))

        # Aggregate results
        df = pd.DataFrame(results)

        summary = {
            'total_tweets': len(df),
            'sentiment_distribution': df['sentiment'].value_counts().to_dict(),
            'average_polarity': df['polarity'].mean(),
            'common_words': self._get_common_words(df['processed_tweet'])
        }

        return df, summary

    def _get_common_words(self, processed_tweets, top_n=10):
        """Get the most common words from processed tweets"""
        words = ' '.join(processed_tweets).split()
        return dict(Counter(words).most_common(top_n))